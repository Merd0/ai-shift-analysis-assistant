#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Destekli Vardiya Devir Analiz AsistanÄ±
Ã‡imento FabrikasÄ± Vardiya Defteri Analizi iÃ§in Optimize EdilmiÅŸ AI Sistemi
"""

import openai
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import json
import re
from config import MODEL_NAME, MAX_TOKENS, TEMPERATURE

class CimentoVardiyaAI:
    def __init__(self, api_key: str = ""):
        """
        Ã‡imento fabrikasÄ± vardiya analizi iÃ§in AI sistemi
        
        Args:
            api_key: OpenAI API key (gÃ¼venlik iÃ§in parametre olarak alÄ±nÄ±r)
        """
        if not api_key:
            raise ValueError("âš ï¸ API Key gerekli! LÃ¼tfen GUI'de API key'inizi girin.")
        
        openai.api_key = api_key
        self.model = MODEL_NAME
        self.max_tokens = MAX_TOKENS
        self.temperature = TEMPERATURE
        
        # Ã‡imento fabrikasÄ± spesifik context
        self.cement_context = self._load_cement_context()
        
    def _load_cement_context(self) -> str:
        """Ã‡imento fabrikasÄ± iÃ§in optimize edilmiÅŸ context"""
        return """
# Ã‡Ä°MENTO FABRÄ°KASI VARDÄ°YA DEFTERÄ° ANALÄ°Z SÄ°STEMÄ°

## AMAÃ‡:
Ã‡imento Ã¼retim sÃ¼recindeki vardiya kayÄ±tlarÄ±nÄ± analiz ederek:
- Ãœretim sorunlarÄ±nÄ± tespit etmek
- Ekipman arÄ±zalarÄ±nÄ± kategorize etmek  
- Ã‡Ã¶zÃ¼m Ã¶nerileri sunmak
- YÃ¶netici raporlarÄ± oluÅŸturmak

## Ã‡Ä°MENTO ÃœRETÄ°M SÃœRECÄ° BÄ°LGÄ°SÄ°:
1. HAMMADDE HAZIRLAMA: KireÃ§taÅŸÄ±, kil, demir cevheri karÄ±ÅŸÄ±mÄ±
2. Ã–ÄÃœTME: Ham karÄ±ÅŸÄ±m Ã¶ÄŸÃ¼tÃ¼cÃ¼lerde (Ã‡D1, Ã‡D2, Ã‡D3, Ã‡D4) ince hale getirilir
3. FIRINDA YAKMA: 1450Â°C'de klinker Ã¼retimi (FÄ±rÄ±n 1, FÄ±rÄ±n 2)
4. Ã‡Ä°MENTO Ã–ÄÃœTME: Klinker + alÃ§Ä± karÄ±ÅŸÄ±mÄ± (Ã‡Ä°M1, Ã‡Ä°M2)
5. DEPOLAMA: Silolarda (Silo 1-8) depolama
6. PAKETLEME/YÃœKLEME: Torba veya dÃ¶kme sevkiyat

## TEMEL EKÄ°PMANLAR:
- Ã‡D1,Ã‡D2,Ã‡D3,Ã‡D4: Ã‡iÄŸ deÄŸirmenler
- Ã‡Ä°M1,Ã‡Ä°M2: Ã‡imento deÄŸirmenleri  
- F1,F2: FÄ±rÄ±nlar
- Silo 1-8: Depolama silolarÄ±
- CSO: Ã‡imento kalite parametreleri
- XRF: Kimyasal analiz cihazÄ±
- Bunker, KonveyÃ¶r, Filtre sistemleri

## SIKÃ‡A YAÅANAN SORUNLAR:
- AÅŸÄ±rÄ± Ä±sÄ±nma (Ã¶zellikle deÄŸirmenlerde)
- Filtre tÄ±kanmasÄ±
- KonveyÃ¶r arÄ±zalarÄ±
- Kalite spek dÄ±ÅŸÄ± deÄŸerler
- Hammadde besleme sorunlarÄ±
- Elektrik kesintileri
- Yedek parÃ§a eksiklikleri

## ANALÄ°Z Ã‡IKTI FORMATI:
1. GÃœNLÃœK Ã–ZET: Ãœretim miktarÄ±, duruÅŸ sÃ¼releri
2. SORUNLAR: YaÅŸanan arÄ±zalar, nedenleri, sÃ¼releri
3. Ã‡Ã–ZÃœMLER: AlÄ±nan aksiyonlar, etkililik
4. Ã–NERÄ°LER: Ã–nleyici bakÄ±m, iyileÅŸtirme Ã¶nerileri
5. TREND: Tekrarlanan sorunlar, risk analizi
"""

    def analyze_shift_data(self, data: pd.DataFrame, date_range: str = "gÃ¼nlÃ¼k", 
                          analysis_options: List[str] = None, user_question: str = "") -> Dict:
        """
        Vardiya verilerini geliÅŸmiÅŸ AI sistemi ile analiz et
        
        Args:
            data: Analiz edilecek vardiya verileri
            date_range: Analiz periyodu ("gÃ¼nlÃ¼k", "haftalÄ±k", vb.)
            analysis_options: Ä°stenilen rapor bÃ¶lÃ¼mleri listesi
            user_question: KullanÄ±cÄ±nÄ±n Ã¶zel sorusu
            
        Returns:
            Dict: AI analiz sonuÃ§larÄ±
        """
        
        # Veriyi Ã¶zetleyerek token tasarrufu
        summary_data = self._summarize_data(data)
        
        # Yeni geliÅŸmiÅŸ AI prompt oluÅŸtur
        prompt = self._create_analysis_prompt(summary_data, date_range, analysis_options, user_question)
        
        # AI analizi Ã§aÄŸÄ±r
        analysis = self._call_openai_api(prompt)
        
        return analysis

    def _summarize_data(self, data: pd.DataFrame) -> str:
        """Veriyi Ã¶zetleyerek token kullanÄ±mÄ±nÄ± optimize et"""
        
        summary_parts = []
        
        # Temel istatistikler
        summary_parts.append(f"ğŸ“Š GENEL BÄ°LGÄ°:")
        summary_parts.append(f"- Toplam kayÄ±t: {len(data)} adet")
        summary_parts.append(f"- Tarih aralÄ±ÄŸÄ±: {data['Tarih'].min()} - {data['Tarih'].max()}")
        
        # AÃ§Ä±klama verilerini Ã¶zetle (en uzun kolonlar genelde aÃ§Ä±klama)
        description_columns = [col for col in data.columns if 
                             any(keyword in col.lower() for keyword in 
                                ['aÃ§Ä±klama', 'iletil', 'takip', 'kalite', 'arÄ±za', 'bakÄ±m'])]
        
        if description_columns:
            summary_parts.append(f"\nğŸ” Ã–NEMLÄ° KAYITLAR:")
            
            # Her kolondaki Ã¶nemli kayÄ±tlarÄ± Ã¶zetle
            for col in description_columns[:3]:  # Sadece ilk 3 kolonu al (token tasarrufu)
                non_empty = data[col].dropna()
                if len(non_empty) > 0:
                    # En uzun ve en kÄ±sa kayÄ±tlarÄ± Ã¶rnek olarak al
                    samples = non_empty.head(5).tolist()  # Sadece 5 Ã¶rnek
                    summary_parts.append(f"\n{col}:")
                    for i, sample in enumerate(samples, 1):
                        # Ã‡ok uzun metinleri kÄ±salt
                        sample_text = str(sample)[:200] + "..." if len(str(sample)) > 200 else str(sample)
                        summary_parts.append(f"  {i}. {sample_text}")
        
        # CSO verilerini Ã¶zetle (varsa)
        cso_columns = [col for col in data.columns if 'cso' in col.lower()]
        if cso_columns:
            summary_parts.append(f"\nğŸ“ˆ KALÄ°TE PARAMETRELERÄ° (CSO):")
            for col in cso_columns[:5]:  # Sadece 5 CSO parametresi
                if data[col].dtype in ['int64', 'float64']:
                    avg_val = data[col].mean()
                    min_val = data[col].min()
                    max_val = data[col].max()
                    summary_parts.append(f"  {col}: Ort={avg_val:.1f}, Min={min_val:.1f}, Max={max_val:.1f}")
        
        return "\n".join(summary_parts)

    def _create_analysis_prompt(self, summary_data: str, date_range: str, analysis_options: List[str] = None, user_question: str = "") -> str:
        """Yeni geliÅŸmiÅŸ prompt sistemi ile analiz prompt'u oluÅŸtur"""
        
        # Yeni prompt sistemini import et
        from prompts import SYSTEM_PROMPT, USER_PROMPT_TEMPLATE
        
        # VarsayÄ±lan analiz seÃ§enekleri
        if analysis_options is None:
            analysis_options = [
                "ğŸ¯ YÃ¶netici Ã–zeti",
                "ğŸ“Š Performans Karnesi", 
                "ğŸ” KÃ¶k Neden Analizi",
                "ğŸ“ˆ Zaman Trendleri ve Risk Tahmini",
                "ğŸ’° Maliyet Etkisi Tahmini",
                "ğŸ’¡ SMART Eylem PlanÄ±",
                "ğŸ“Œ YÃ¶netici Aksiyon Panosu"
            ]
        
        # Analiz seÃ§eneklerini formatla
        formatted_options = "\n".join([f"âœ… {option}" for option in analysis_options])
        
        # Yeni prompt template'ini kullan
        user_prompt = USER_PROMPT_TEMPLATE.format(
            data_summary=summary_data,
            analysis_options=formatted_options,
            user_question=user_question if user_question else "Yok"
        )
        
        # System prompt + User prompt kombinasyonu
        full_prompt = f"{SYSTEM_PROMPT}\n\n{user_prompt}"
        
        return full_prompt

    def _call_openai_api(self, prompt: str) -> Dict:
        """OpenAI API Ã§aÄŸrÄ±sÄ± - optimize edilmiÅŸ"""
        
        try:
            client = openai.OpenAI(api_key=openai.api_key)
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user", 
                        "content": prompt  # Prompt zaten system + user iÃ§eriyor
                    }
                ],
                max_tokens=self.max_tokens,
                temperature=0.8,  # YaratÄ±cÄ±lÄ±ÄŸÄ± artÄ±r
                top_p=0.95,  # Ã‡eÅŸitliliÄŸi artÄ±r
                frequency_penalty=0.6,  # TekrarlarÄ± gÃ¼Ã§lÃ¼ ÅŸekilde azalt
                presence_penalty=0.6   # Yeni konularÄ± teÅŸvik et
            )
            
            analysis_text = response.choices[0].message.content
            
            # YanÄ±tÄ± yapÄ±landÄ±rÄ±lmÄ±ÅŸ formata Ã§evir (ÅŸimdilik basit format)
            structured_analysis = analysis_text
            
            # Token kullanÄ±mÄ±nÄ± logla
            token_usage = {
                'prompt_tokens': response.usage.prompt_tokens,
                'completion_tokens': response.usage.completion_tokens,
                'total_tokens': response.usage.total_tokens,
                'estimated_cost': response.usage.total_tokens * 0.00015 / 1000  # GPT-4o-mini fiyatÄ±
            }
            
            return {
                'analysis': structured_analysis,
                'raw_response': analysis_text,
                'token_usage': token_usage,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                'error': f"AI analizi hatasÄ±: {str(e)}",
                'analysis': None,
                'token_usage': None,
                'timestamp': datetime.now().isoformat()
            }

    def _parse_analysis_response(self, response_text: str) -> Dict:
        """AI yanÄ±tÄ±nÄ± yapÄ±landÄ±rÄ±lmÄ±ÅŸ formata Ã§evir"""
        
        sections = {
            'gÃ¼nlÃ¼k_Ã¶zet': '',
            'sorunlar': [],
            'Ã§Ã¶zÃ¼mler': [],
            'Ã¶neriler': [],
            'trend_analizi': ''
        }
        
        # Basit parsing (regex ile bÃ¶lÃ¼mleri ayÄ±r)
        current_section = None
        lines = response_text.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # BÃ¶lÃ¼m baÅŸlÄ±klarÄ±nÄ± tespit et
            if 'ğŸ“Š' in line or 'GÃœNLÃœK Ã–ZET' in line:
                current_section = 'gÃ¼nlÃ¼k_Ã¶zet'
                continue
            elif 'âš ï¸' in line or 'SORUNLAR' in line:
                current_section = 'sorunlar'
                continue
            elif 'âœ…' in line or 'Ã‡Ã–ZÃœMLER' in line:
                current_section = 'Ã§Ã¶zÃ¼mler'
                continue
            elif 'ğŸ¯' in line or 'Ã–NERÄ°LER' in line:
                current_section = 'Ã¶neriler'
                continue
            elif 'ğŸ“ˆ' in line or 'TREND' in line:
                current_section = 'trend_analizi'
                continue
            
            # Ä°Ã§eriÄŸi ilgili bÃ¶lÃ¼me ekle
            if current_section:
                if current_section in ['sorunlar', 'Ã§Ã¶zÃ¼mler', 'Ã¶neriler']:
                    if line.startswith('-') or line.startswith('â€¢'):
                        sections[current_section].append(line[1:].strip())
                else:
                    sections[current_section] += line + '\n'
        
        return sections

    def generate_manager_report(self, analysis: Dict, period: str = "gÃ¼nlÃ¼k") -> str:
        """YÃ¶netici iÃ§in Ã¶zet rapor oluÅŸtur"""
        
        if analysis.get('error'):
            return f"âŒ Rapor oluÅŸturulamadÄ±: {analysis['error']}"
        
        structured = analysis['analysis']
        token_info = analysis.get('token_usage', {})
        
        report = f"""
ğŸ­ Ã‡Ä°MENTO FABRÄ°KASI VARDÄ°YA RAPORU
{'='*50}
ğŸ“… Rapor Tipi: {period.title()}
ğŸ• OluÅŸturma: {datetime.now().strftime('%d.%m.%Y %H:%M')}

{structured.get('gÃ¼nlÃ¼k_Ã¶zet', 'Ã–zet bulunamadÄ±')}

âš ï¸ KRÄ°TÄ°K SORUNLAR ({len(structured.get('sorunlar', []))} adet):
"""
        
        for i, sorun in enumerate(structured.get('sorunlar', [])[:5], 1):  # Sadece ilk 5 sorun
            report += f"{i}. {sorun}\n"
        
        report += f"""
ğŸ¯ Ã–NCELIKLI AKSIYONLAR ({len(structured.get('Ã¶neriler', []))} adet):
"""
        
        for i, Ã¶neri in enumerate(structured.get('Ã¶neriler', [])[:3], 1):  # Sadece ilk 3 Ã¶neri
            report += f"{i}. {Ã¶neri}\n"
        
        if token_info:
            report += f"""
ğŸ“Š SÄ°STEM BÄ°LGÄ°SÄ°:
- Token kullanÄ±mÄ±: {token_info.get('total_tokens', 0)}
- Tahmini maliyet: ${token_info.get('estimated_cost', 0):.4f}
"""
        
        return report

    def save_analysis(self, analysis: Dict, filename: str = None) -> str:
        """Analizi dosyaya kaydet"""
        
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"vardiya_analizi_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(analysis, f, ensure_ascii=False, indent=2)
            return filename
        except Exception as e:
            return f"KayÄ±t hatasÄ±: {str(e)}"


# Test fonksiyonu
def test_ai_system():
    """AI sistemini test et"""
    print("ğŸ¤– AI Vardiya Analiz Sistemi Test Ediliyor...")
    
    ai = CimentoVardiyaAI()
    
    # Ã–rnek veri oluÅŸtur (gerÃ§ek veri yerine)
    sample_data = pd.DataFrame({
        'Tarih': ['2025-08-07', '2025-08-07', '2025-08-07'],
        'Vardiya': ['07:00-15:00', '15:00-23:00', '23:00-07:00'],
        'AÃ§Ä±klama': [
            'Ã‡D2 saat 10:30da durdu. AÅŸÄ±rÄ± Ä±sÄ±nma nedeniyle. SoÄŸutma sistemi kontrol edildi.',
            'FÄ±rÄ±n 1 normal Ã§alÄ±ÅŸÄ±yor. CSO deÄŸerleri spek iÃ§inde.',
            'Ã‡Ä°M2 Ã¶ÄŸÃ¼tÃ¼cÃ¼sÃ¼nde titreÅŸim var. BakÄ±m ekibi bilgilendirildi.'
        ],
        'CSO 1': [2.1, 2.3, 2.2],
        'CSO 2': [3.1, 3.0, 3.2]
    })
    
    # Analiz et
    result = ai.analyze_shift_data(sample_data)
    
    if result.get('error'):
        print(f"âŒ Hata: {result['error']}")
        return
    
    # YÃ¶netici raporu oluÅŸtur
    manager_report = ai.generate_manager_report(result)
    print(manager_report)
    
    # Dosyaya kaydet
    saved_file = ai.save_analysis(result)
    print(f"\nğŸ’¾ Analiz kaydedildi: {saved_file}")


if __name__ == "__main__":
    test_ai_system()
