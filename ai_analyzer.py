#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Destekli Vardiya Devir Analiz AsistanÄ±
Ã‡imento FabrikasÄ± Vardiya Defteri Analizi iÃ§in Optimize EdilmiÅŸ AI Sistemi
"""

from openai import OpenAI
import requests
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import json
import re
from config import MODEL_NAME, MAX_TOKENS, TEMPERATURE

class CimentoVardiyaAI:
    def __init__(self, api_key: str = "", provider: str = "openai", model: Optional[str] = None, base_url: Optional[str] = None,
                 max_tokens: Optional[int] = None, temperature: Optional[float] = None):
        """
        Ã‡imento fabrikasÄ± vardiya analizi iÃ§in AI sistemi
        
        Args:
            api_key: OpenAI API key (gÃ¼venlik iÃ§in parametre olarak alÄ±nÄ±r)
        """
        if not api_key:
            raise ValueError("âš ï¸ API Key gerekli! LÃ¼tfen GUI'de API key'inizi girin.")

        # SaÄŸlayÄ±cÄ± seÃ§imi
        self.provider = provider.lower().strip()
        self.api_key = api_key

        # OpenAI istemcisi (varsayÄ±lan)
        self.client = None
        if self.provider == "openai":
            self.client = OpenAI(api_key=api_key)

        # Model ve jenerasyon ayarlarÄ±
        self.model = model or MODEL_NAME
        self.max_tokens = int(max_tokens if max_tokens is not None else MAX_TOKENS)
        self.temperature = float(temperature if temperature is not None else TEMPERATURE)
        
        # Ã‡imento fabrikasÄ± spesifik context
        self.cement_context = self._load_cement_context()

        # SaÄŸlayÄ±cÄ±ya gÃ¶re tahmini baÄŸlam limitleri (token)
        self._context_limits = {
            "openai": 128000,
            "anthropic": 200000,
            "xai": 131072
        }

    def _approx_tokens(self, text: str) -> int:
        """Basit yaklaÅŸÄ±k token hesabÄ± (karakter/4)."""
        if not text:
            return 0
        return max(1, int(len(text) / 4))

    def _auto_adjust_generation_params(self, prompt: str, data_rows: int) -> None:
        """SatÄ±r sayÄ±sÄ± ve prompt uzunluÄŸuna gÃ¶re max_tokens/sÄ±caklÄ±k ayarÄ± yap."""
        # BaÄŸlam limiti ve gÃ¼venli boÅŸluk (%20 buffer)
        limit = self._context_limits.get(self.provider, 128000)
        prompt_tokens = self._approx_tokens(prompt)
        safe_room = int(limit * 0.8) - prompt_tokens
        safe_room = max(512, safe_room)

        # SatÄ±r bazlÄ± hedef (Ã§ok uzun raporlar iÃ§in)
        if data_rows < 1000:
            row_target = 4000
        elif data_rows < 5000:
            row_target = 8000
        elif data_rows < 15000:
            row_target = 12000
        else:
            row_target = 16000

        requested = self.max_tokens
        target = max(requested, row_target)
        # Ãœst sÄ±nÄ±r ve gÃ¼venli baÄŸlam kÄ±sÄ±tÄ±
        target = min(target, safe_room, 20000)
        self.max_tokens = max(512, int(target))

        # Ã‡ok bÃ¼yÃ¼k veri iÃ§in sÄ±caklÄ±ÄŸÄ± bir miktar aÅŸaÄŸÄ± Ã§ek (tutarlÄ±lÄ±k)
        if data_rows >= 5000:
            self.temperature = min(self.temperature, 0.6)
        
    def _load_cement_context(self) -> str:
        """Ã‡imento fabrikasÄ± iÃ§in optimize edilmiÅŸ context"""
        return """
# Ã‡Ä°MENTO FABRÄ°KASI VARDÄ°YA DEFTERÄ° ANALÄ°Z SÄ°STEMÄ°

## AMAÃ‡:
Ã‡imento Ã¼retim sÃ¼recindeki vardiya kayÄ±tlarÄ±nÄ± analiz ederek:
- Ãœretim sorunlarÄ±nÄ± tespit etmek
- Ekipman arÄ±zalarÄ±nÄ± kategorize etmek  
- Ã‡Ã¶zÃ¼m Ã¶nerileri sunmak
- YÃ¶netici raporlarÄ± oluÅŸturmak

## Ã‡Ä°MENTO ÃœRETÄ°M SÃœRECÄ° BÄ°LGÄ°SÄ°:
1. HAMMADDE HAZIRLAMA: KireÃ§taÅŸÄ±, kil, demir cevheri karÄ±ÅŸÄ±mÄ±
2. Ã–ÄÃœTME: Ham karÄ±ÅŸÄ±m Ã¶ÄŸÃ¼tÃ¼cÃ¼lerde (Ã‡D1, Ã‡D2, Ã‡D3, Ã‡D4) ince hale getirilir
3. FIRINDA YAKMA: 1450Â°C'de klinker Ã¼retimi (FÄ±rÄ±n 1, FÄ±rÄ±n 2)
4. Ã‡Ä°MENTO Ã–ÄÃœTME: Klinker + alÃ§Ä± karÄ±ÅŸÄ±mÄ± (Ã‡Ä°M1, Ã‡Ä°M2)
5. DEPOLAMA: Silolarda (Silo 1-8) depolama
6. PAKETLEME/YÃœKLEME: Torba veya dÃ¶kme sevkiyat

## TEMEL EKÄ°PMANLAR:
- Ã‡D1,Ã‡D2,Ã‡D3,Ã‡D4: Ã‡iÄŸ deÄŸirmenler
- Ã‡Ä°M1,Ã‡Ä°M2: Ã‡imento deÄŸirmenleri  
- F1,F2: FÄ±rÄ±nlar
- Silo 1-8: Depolama silolarÄ±
- CSO: Ã‡imento kalite parametreleri
- XRF: Kimyasal analiz cihazÄ±
- Bunker, KonveyÃ¶r, Filtre sistemleri

## SIKÃ‡A YAÅANAN SORUNLAR:
- AÅŸÄ±rÄ± Ä±sÄ±nma (Ã¶zellikle deÄŸirmenlerde)
- Filtre tÄ±kanmasÄ±
- KonveyÃ¶r arÄ±zalarÄ±
- Kalite spek dÄ±ÅŸÄ± deÄŸerler
- Hammadde besleme sorunlarÄ±
- Elektrik kesintileri
- Yedek parÃ§a eksiklikleri

## ANALÄ°Z Ã‡IKTI FORMATI:
1. GÃœNLÃœK Ã–ZET: Ãœretim miktarÄ±, duruÅŸ sÃ¼releri
2. SORUNLAR: YaÅŸanan arÄ±zalar, nedenleri, sÃ¼releri
3. Ã‡Ã–ZÃœMLER: AlÄ±nan aksiyonlar, etkililik
4. Ã–NERÄ°LER: Ã–nleyici bakÄ±m, iyileÅŸtirme Ã¶nerileri
5. TREND: Tekrarlanan sorunlar, risk analizi
"""

    def analyze_shift_data(self, data: pd.DataFrame, date_range: str = "gÃ¼nlÃ¼k", 
                          analysis_options: List[str] = None, user_question: str = "") -> Dict:
        """
        Vardiya verilerini geliÅŸmiÅŸ AI sistemi ile analiz et
        
        Args:
            data: Analiz edilecek vardiya verileri
            date_range: Analiz periyodu ("gÃ¼nlÃ¼k", "haftalÄ±k", vb.)
            analysis_options: Ä°stenilen rapor bÃ¶lÃ¼mleri listesi
            user_question: KullanÄ±cÄ±nÄ±n Ã¶zel sorusu
            
        Returns:
            Dict: AI analiz sonuÃ§larÄ±
        """
        
        # Veriyi Ã¶zetleyerek token tasarrufu
        summary_data = self._summarize_data(data)
        
        # Yeni geliÅŸmiÅŸ AI prompt oluÅŸtur
        prompt = self._create_analysis_prompt(summary_data, date_range, analysis_options, user_question)

        # Token/sÄ±caklÄ±k otomatik ayarÄ±
        try:
            data_rows = int(len(data)) if data is not None else 0
        except Exception:
            data_rows = 0
        self._auto_adjust_generation_params(prompt, data_rows)
        
        # AI analizi Ã§aÄŸÄ±r
        analysis = self._call_llm_api(prompt)
        
        return analysis

    def _summarize_data(self, data: pd.DataFrame) -> str:
        """Veriyi zengin ÅŸekilde Ã¶zetleyip AI'a gÃ¼Ã§lÃ¼ baÄŸlam saÄŸla (KPI + trend + top listeler).

        AyrÄ±ca bazÄ± daÄŸÄ±lÄ±mlarÄ± Ã¶nceden hesaplayÄ±p yÃ¼zde toplamÄ±nÄ± %100'e normalize eder.
        """

        lines: List[str] = []

        def _clean_count_series(series: pd.Series) -> pd.Series:
            """Kategori sayÄ±mlarÄ± iÃ§in metin serisini temizle.
            - BoÅŸ / NaN / None / Null / N/A / NA / NaT / '-' gibi deÄŸerleri Ã§Ä±kar
            - AÅŸÄ±rÄ± boÅŸluklarÄ± normalize et
            """
            try:
                s = series.astype(str).str.strip()
                # Lower-case kopya ile null benzerlerini tespit et
                s_lower = s.str.lower()
                null_like = [
                    '', 'nan', 'none', 'null', 'nat', 'n/a', 'na', 'n\\a', 'n.a', 'n.a.', '-', '--', 'â€”', 'yok', 'bilinmiyor'
                ]
                mask = s_lower.isin(null_like)
                s = s[~mask]
                # Tek karakterlik anlamsÄ±z deÄŸerleri filtrele (Ã¶rn: '.')
                s = s[s.str.len() >= 2]
                # Whitespace normalizasyonu
                s = s.str.replace(r"\s+", " ", regex=True)
                return s
            except Exception:
                return series.dropna()

        # Tarih bilgisi (varsa)
        date_col_candidates = [c for c in data.columns if any(k in c.lower() for k in ['tarih', 'date'])]
        date_range_text = "N/A"
        if date_col_candidates:
            dc = date_col_candidates[0]
            try:
                dates = pd.to_datetime(data[dc], errors='coerce')
                min_d = dates.min()
                max_d = dates.max()
                if pd.notna(min_d) and pd.notna(max_d):
                    date_range_text = f"{min_d.date()} - {max_d.date()}"
            except Exception:
                pass

        # Genel
        lines.append("ğŸ“Š GENEL BÄ°LGÄ°:")
        lines.append(f"- Toplam kayÄ±t: {len(data)}")
        lines.append(f"- Tarih aralÄ±ÄŸÄ±: {date_range_text}")

        # GÃ¼ncellik daÄŸÄ±lÄ±mÄ±: son 90/180/365 gÃ¼n ve 24+ ay Ã¶nceki kayÄ±t sayÄ±larÄ±
        if date_col_candidates:
            try:
                dc = date_col_candidates[0]
                dates = pd.to_datetime(data[dc], errors='coerce')
                now = pd.Timestamp.now(tz=None).normalize()
                last90 = (dates >= now - pd.Timedelta(days=90)).sum()
                last180 = (dates >= now - pd.Timedelta(days=180)).sum()
                last365 = (dates >= now - pd.Timedelta(days=365)).sum()
                older24m = (dates < now - pd.Timedelta(days=730)).sum()
                lines.append("- GÃ¼ncellik (kayÄ±t adedi): son 90g=%d | 180g=%d | 365g=%d | 24+ ay=%d" % (int(last90), int(last180), int(last365), int(older24m)))
                if older24m and last365 == 0:
                    lines.append("- Not: KayÄ±tlarÄ±n Ã§oÄŸu 24+ ay Ã¶ncesi. Eylem planÄ± Ã¼retimi sÄ±nÄ±rlÄ± tutulacaktÄ±r.")
            except Exception:
                pass

        # Vardiya daÄŸÄ±lÄ±mÄ± (normalize + Ã¶rnekleme hatalarÄ±na dayanÄ±klÄ±)
        shift_col = next((c for c in data.columns if 'vardiya' in c.lower()), None)
        if shift_col is not None:
            try:
                vc = _clean_count_series(data[shift_col]).value_counts()
                dist = vc.head(10)
                lines.append("\nğŸ•’ VARDÄ°YA DAÄILIMI (ilk 10):")
                for k, v in dist.items():
                    lines.append(f"- {k}: {int(v)}")
            except Exception:
                pass

        # Normalize edici yardÄ±mcÄ±
        def _normalized_percentages(vc: pd.Series, top_n: int = 10) -> List[Tuple[str, int, int]]:
            """value_counts serisini ilk top_n iÃ§in (ad, adet, %) ve bir 'DiÄŸer' ile %100'e
            tamamlayarak dÃ¶ndÃ¼rÃ¼r. YÃ¼zdeler tamsayÄ±ya yuvarlanÄ±r ve son kaleme fark eklenir.
            """
            items = vc.head(top_n)
            total = int(vc.sum()) if vc.sum() else 0
            result: List[Tuple[str, int, int]] = []
            if total == 0:
                return result
            percents = []
            names = list(items.index.astype(str))
            counts = list(items.astype(int).values)
            for c in counts:
                percents.append(int(round(c * 100.0 / total)))
            diff = 100 - sum(percents)
            if percents:
                percents[-1] += diff
            for n, c, p in zip(names, counts, percents):
                result.append((n, int(c), int(p)))
            # DiÄŸer
            others = total - sum(counts)
            if others > 0:
                p_other = max(0, 100 - sum([p for _, _, p in result]))
                result.append(("DiÄŸer", int(others), int(p_other)))
            return result

        # Ekipman daÄŸÄ±lÄ±mÄ±
        equipment_col = next((c for c in data.columns if any(k in c.lower() for k in ['ekipman', 'makine', 'Ã¼nite', 'unite', 'unit'])), None)
        if equipment_col is not None:
            try:
                vc = _clean_count_series(data[equipment_col]).value_counts()
                lines.append("\nğŸ­ EKÄ°PMAN DAÄILIMI (ilk 10, normalize):")
                dist = _normalized_percentages(vc, top_n=10)
                for name, cnt, pct in dist:
                    lines.append(f"- {name}: {cnt} kayÄ±t (%{pct})")
                if vc.sum() > 0:
                    lines.append("Toplam = %100")
            except Exception:
                pass

        # Sorun / Kategori daÄŸÄ±lÄ±mÄ±
        issue_col = next((c for c in data.columns if any(k in c.lower() for k in ['sorun', 'arÄ±za', 'ariza', 'problem', 'kategori'])), None)
        if issue_col is not None:
            try:
                vc = _clean_count_series(data[issue_col]).str.lower().value_counts()
                lines.append("\nâš ï¸ SORUN KATEGORÄ°LERÄ° (ilk 10, normalize):")
                dist = _normalized_percentages(vc, top_n=10)
                for name, cnt, pct in dist:
                    lines.append(f"- {name}: {cnt} kayÄ±t (%{pct})")
                if vc.sum() > 0:
                    lines.append("Toplam = %100")
            except Exception:
                pass

        # DuruÅŸ/SÃ¼re (dakika) + MTBF/MTTR hesaplamasÄ±na uygunluk
        duration_col = next((c for c in data.columns if any(k in c.lower() for k in ['sÃ¼re', 'sure', 'dakika', 'dk'])), None)
        if duration_col is not None:
            try:
                # Metin iÃ§indeki sayÄ±larÄ± da yakalamaya Ã§alÄ±ÅŸ (Ã¶r. "45 dk", "~30")
                cleaned = (
                    data[duration_col]
                    .astype(str)
                    .str.extract(r'(\d+[\.,]?\d*)', expand=False)
                )
                durations = pd.to_numeric(cleaned.str.replace(',', '.', regex=False), errors='coerce')
                total_min = durations.fillna(0).sum()
                avg_min = durations.dropna().mean() if durations.notna().any() else 0
                lines.append("\nâ±ï¸ DuruÅŸ SÃ¼resi (dakika):")
                lines.append(f"- Toplam: {int(total_min)} dk")
                lines.append(f"- Ortalama: {avg_min:.1f} dk/kayÄ±t")
                # MTBF/MTTR iÃ§in veri uygunluk sinyali (Ã¶rnek: tarih/baÅŸlangÄ±Ã§-bitiÅŸ yoksa hesaplama yapÄ±lmaz)
                lines.append("- MTBF/MTTR: veri varsa hesaplanÄ±r; eksikse 'veri yok' yaz")

                # HaftalÄ±k ortalama duruÅŸ sÃ¼resi (son 7 gÃ¼n vs Ã¶nceki 7 gÃ¼n)
                if date_col_candidates:
                    try:
                        dc = date_col_candidates[0]
                        dates = pd.to_datetime(data[dc], errors='coerce')
                        df_tmp = pd.DataFrame({
                            'date': dates.dt.normalize(),
                            'dur_min': durations
                        })
                        now_d = pd.Timestamp.now().normalize()
                        last7_mask = df_tmp['date'] >= (now_d - pd.Timedelta(days=7))
                        prev7_mask = (df_tmp['date'] < (now_d - pd.Timedelta(days=7))) & (df_tmp['date'] >= (now_d - pd.Timedelta(days=14)))
                        mean_last7 = df_tmp.loc[last7_mask, 'dur_min'].dropna().mean()
                        mean_prev7 = df_tmp.loc[prev7_mask, 'dur_min'].dropna().mean()
                        if pd.notna(mean_prev7) or pd.notna(mean_last7):
                            last7_text = f"{mean_last7:.1f} dk" if pd.notna(mean_last7) else "veri yok"
                            prev7_text = f"{mean_prev7:.1f} dk" if pd.notna(mean_prev7) else "veri yok"
                            lines.append("- HaftalÄ±k ortalama (dk/kayÄ±t): geÃ§en hafta = %s | bu hafta = %s" % (prev7_text, last7_text))
                    except Exception:
                        pass
            except Exception:
                pass

        # Trend Ã¶zeti (son 7 gÃ¼n vs Ã¶nceki 7 gÃ¼n)
        if date_col_candidates:
            dc = date_col_candidates[0]
            try:
                df_copy = data.copy()
                df_copy[dc] = pd.to_datetime(df_copy[dc], errors='coerce')
                daily_counts = df_copy.groupby(df_copy[dc].dt.date).size().sort_index()
                if len(daily_counts) >= 14:
                    last7 = daily_counts[-7:].sum()
                    prev7 = daily_counts[-14:-7].sum()
                    delta = last7 - prev7
                    trend = "â†‘" if delta > 0 else ("â†“" if delta < 0 else "=")
                    lines.append("\nğŸ“ˆ Trend (kayÄ±t adedi):")
                    lines.append(f"- Son 7 gÃ¼n: {int(last7)} | Ã–nceki 7: {int(prev7)} | Fark: {int(delta)} {trend}")
            except Exception:
                pass

        # AÃ§Ä±klamalardan kÄ±sa Ã¶rnekler
        description_columns = [col for col in data.columns if any(keyword in col.lower() for keyword in ['aÃ§Ä±klama', 'aciklama', 'iletil', 'takip', 'kalite', 'arÄ±za', 'ariza', 'bakÄ±m', 'bakim', 'not', 'yorum'])]
        if description_columns:
            lines.append("\nğŸ” Ã–RNEK KAYITLAR (max 3 kolon x 3 Ã¶rnek):")
            for col in description_columns[:3]:
                non_empty = data[col].dropna().astype(str)
                if len(non_empty) > 0:
                    samples = non_empty.head(3).tolist()
                    lines.append(f"- {col}:")
                    for i, sample in enumerate(samples, 1):
                        sample_text = sample[:200] + "..." if len(sample) > 200 else sample
                        lines.append(f"  {i}. {sample_text}")

        return "\n".join(lines)

    def _create_analysis_prompt(self, summary_data: str, date_range: str, analysis_options: List[str] = None, user_question: str = "") -> str:
        """Yeni geliÅŸmiÅŸ prompt sistemi ile analiz prompt'u oluÅŸtur"""
        
        # Yeni prompt sistemini import et
        from prompts import SYSTEM_PROMPT, USER_PROMPT_TEMPLATE
        
        # VarsayÄ±lan analiz seÃ§enekleri
        if analysis_options is None:
            analysis_options = [
                "ğŸ¯ YÃ¶netici Ã–zeti",
                "ğŸ“Š Performans Karnesi", 
                "ğŸ” KÃ¶k Neden Analizi",
                "ğŸ“ˆ Zaman Trendleri ve Risk Tahmini",
                "ğŸ’¡ SMART Eylem PlanÄ±",
                "ğŸ“Œ YÃ¶netici Aksiyon Panosu"
            ]
        
        # Analiz seÃ§eneklerini formatla
        formatted_options = "\n".join([f"âœ… {option}" for option in analysis_options])
        
        # Yeni prompt template'ini kullan
        user_prompt = USER_PROMPT_TEMPLATE.format(
            data_summary=summary_data,
            analysis_options=formatted_options,
            user_question=user_question if user_question else "Yok"
        )
        
        # Ã‡Ä±ktÄ± kÄ±sÄ±tlarÄ± (halÃ¼sinasyon Ã¶nleme + uzunluk kontrolÃ¼)
        constraints = f"""
\n---\n
ğŸ”’ Ã‡IKTI KISITLARI (Kesin Uyman Gerekir)
- Maksimum yanÄ±t uzunluÄŸu: {self.max_tokens} token (gereksiz tekrar, uzun alÄ±ntÄ± yok)
- Veri dÄ±ÅŸÄ± iddia Ã¼retme; belirsizse "veri yok" de
- Finansal rakam, TL/USD/â‚º, ROI vb. UYDURMA; geÃ§erse kaldÄ±r
- DÄ±ÅŸ link, resim/grafik embed etme; dÃ¼z metin ve gerekirse ASCII tablo
- YÃ¼zdeler Toplam = %100 (Â±1), aksi durumda normalleÅŸtir ve belirt
 - Eylem PlanÄ± bÃ¶lÃ¼mlerinde placeholder/boÅŸ satÄ±r kullanma ("...", "devam eden Ã¶neriler" vb. YASAK)
  - Eylem PlanÄ± Ã¶neri adedi dinamik; sadece son 12 ayda yinelenen/etkisi sÃ¼ren sorunlara aksiyon Ã¼ret. 24+ ay Ã¶nceki mÃ¼nferit olaylara aksiyon yazma; gerekiyorsa "tarih eski â€” doÄŸrulama/izleme" notu ekle.
  - Eylem formatÄ±: [Ã–neri] â€“ Dayanak veri (N/%, sÃ¼re, tarih aralÄ±ÄŸÄ±) â€“ Sorumlu â€“ BaÅŸarÄ± metriÄŸi â€“ Ã–ncelik(1-10) â€“ Zorluk(Kolay/Orta/Zor) â€“ SÃ¼re
 - YÃ¶netici Ã–zetinde 8-15 kritik bulgu; gerekiyorsa daha fazla. SÄ±rala: frekans, sÃ¼re ve etki.
"""

        # System prompt + User prompt + kÄ±sÄ±tlar
        full_prompt = f"{SYSTEM_PROMPT}\n\n{user_prompt}\n{constraints}"
        
        return full_prompt

    def _call_llm_api(self, prompt: str) -> Dict:
        """SeÃ§ili saÄŸlayÄ±cÄ±ya gÃ¶re API Ã§aÄŸrÄ±sÄ±"""

        try:
            if self.provider == "openai":
                response = self.client.chat.completions.create(
                model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                    top_p=0.9,
                    frequency_penalty=0.7,
                    presence_penalty=0.4,
                )
                analysis_text = response.choices[0].message.content
                token_usage = {
                    'prompt_tokens': getattr(response.usage, 'prompt_tokens', None),
                    'completion_tokens': getattr(response.usage, 'completion_tokens', None),
                    'total_tokens': getattr(response.usage, 'total_tokens', None),
                }

            elif self.provider == "anthropic":
                # Claude Messages API
                url = "https://api.anthropic.com/v1/messages"
                headers = {
                    "x-api-key": self.api_key,
                    "anthropic-version": "2023-06-01",
                    "content-type": "application/json"
                }
                payload = {
                    "model": self.model,
                    "max_tokens": self.max_tokens,
                    "temperature": self.temperature,
                    "messages": [{"role": "user", "content": prompt}]
                }
                r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=60)
                r.raise_for_status()
                data = r.json()
                # Claude yanÄ±tÄ±
                parts = data.get("content", [])
                analysis_text = "".join([p.get("text", "") for p in parts]) if isinstance(parts, list) else data.get("content", "")
                token_usage = data.get("usage", {})

            elif self.provider == "xai":
                # xAI Grok (OpenAI uyumlu style olabilir; burada basit REST Ã¶rneÄŸi)
                base = "https://api.x.ai/v1"
                url = f"{base}/chat/completions"
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                }
                payload = {
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": self.max_tokens,
                    "temperature": self.temperature
                }
                r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=60)
                r.raise_for_status()
                data = r.json()
                analysis_text = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                token_usage = data.get("usage", {})

            else:
                raise ValueError(f"Desteklenmeyen saÄŸlayÄ±cÄ±: {self.provider}")

            analysis_text = self._sanitize_response(analysis_text)
            structured_analysis = self._parse_analysis_response(analysis_text)
            return {
                'analysis': structured_analysis,
                'raw_response': analysis_text,
                'token_usage': token_usage,
                'timestamp': datetime.now().isoformat()
            }
        except Exception as e:
            # max_tokens/context hatasÄ±nda otomatik dÃ¼ÅŸÃ¼r ve yeniden dene
            msg = str(e).lower()
            if any(k in msg for k in ["max_token", "context", "too many tokens", "reduce"]):
                try:
                    new_max = max(512, int(self.max_tokens * 0.75))
                    if new_max == self.max_tokens:
                        new_max = max(512, self.max_tokens - 512)
                    self.max_tokens = new_max
                    return self._call_llm_api(prompt)
                except Exception:
                    pass
            return {
                'error': f"AI analizi hatasÄ±: {str(e)}",
                'analysis': None,
                'token_usage': None,
                'timestamp': datetime.now().isoformat()
            }

    def _sanitize_response(self, text: str) -> str:
        """Basit halÃ¼sinasyon ve biÃ§im temizliÄŸi: para/URL kaldÄ±r, aÅŸÄ±rÄ± uzunluÄŸu kes."""
        if not text:
            return text
        try:
            # URL'ler
            text = re.sub(r"https?://\S+", "", text)
            # Para birimleri
            text = re.sub(r"(\â‚º|\$|USD|TL|TRY|EUR|â‚¬)", "", text, flags=re.IGNORECASE)
            # Yinelenen boÅŸ satÄ±rlarÄ± sadeleÅŸtir
            text = re.sub(r"\n{3,}", "\n\n", text)
            # YÃ¼zde %0 sorunlarÄ±nÄ± Ã§Ã¶z
            text = re.sub(r"\(%\s*0\s*\)", "(â‰ˆ%<1)", text)
            text = re.sub(r"%\s*0\b", "â‰ˆ%<1", text)
            text = re.sub(r"(\d+)\s*\(\s*%\s*0\s*\)", r"\1 (â‰ˆ%<1)", text)
            # Placeholder X/Y saat|dk -> veri yok
            text = re.sub(r"=\s*[XYxy]\s*(saat|dk|dakika)", "= veri yok", text)
            text = re.sub(r"\b[XYxy]\s*(saat|dk|dakika)\b", "veri yok", text)
            # Dayanak veri temizliÄŸi - daha kapsamlÄ±
            text = re.sub(r"(?i)Dayanak\s*veri\s*:\s*(N/?A|NA|N\.A\.?|NONE|null|eksik|yok|boÅŸ)\b", "Dayanak veri: veri yok", text)
            text = re.sub(r"(?i)Dayanak\s*veri\s*:\s*veri\s*yok\s*â€”", "Dayanak veri: veri yok â€”", text)
            # Ã‡ok uzun Dayanak veri satÄ±rlarÄ±nÄ± kÄ±salt
            text = re.sub(r"(?i)(Dayanak\s*veri\s*:\s*veri\s*yok)[\sâ€”]*([^â€”]*â€”[^â€”]*â€”[^â€”]*â€”[^â€”]*â€”[^â€”]*)", r"\1 â€” \2", text)
            # Eylem planÄ± alan adlarÄ±nda bozulma: '-soru-' tekrarlarÄ±nÄ± dÃ¼zelt
            text = re.sub(r"(?i)(?:[\-â€”]\s*soru\s*){2,}", " â€” Sorumlu â€” ", text)
            text = re.sub(r"(?i)(?<=[-â€”])\s*soru\s*(?=[-â€”])", " Sorumlu ", text)
            # SatÄ±r limiti (Ã§ok uzun raporlarÄ± sÄ±nÄ±rlÄ± tut)
            lines = text.splitlines()
            new_lines = []
            for line in lines:
                # MTBF/MTTR yer tutucu/uygunsuz deÄŸerleri bastÄ±r
                if re.search(r"\bMTBF\b|\bMTTR\b", line, flags=re.IGNORECASE):
                    # SayÄ± var mÄ±? X/Y/N/A/NA/dash varsa veri yok kabul et
                    if not re.search(r"\d", line) or re.search(r"\b(X|Y|N/?A)\b|--|â€”", line, flags=re.IGNORECASE):
                        line = re.sub(r":.*$", ": veri yok (zaman damgalÄ± arÄ±za/onarÄ±m verisi eksik)", line)
                new_lines.append(line)
            if len(new_lines) > 4000:
                new_lines = new_lines[:4000] + ["... [Ã§Ä±ktÄ± kÄ±saltÄ±ldÄ±]"]
            text = "\n".join(new_lines)
            # Karakter Ã¼st limiti
            if len(text) > 120000:
                text = text[:120000] + "\n... [Ã§Ä±ktÄ± kÄ±saltÄ±ldÄ±]"
            return text
        except Exception:
            return text

    def _parse_analysis_response(self, response_text: str) -> Dict:
        """AI yanÄ±tÄ±nÄ± yapÄ±landÄ±rÄ±lmÄ±ÅŸ formata Ã§evir"""
        
        sections = {
            'gÃ¼nlÃ¼k_Ã¶zet': '',
            'sorunlar': [],
            'Ã§Ã¶zÃ¼mler': [],
            'Ã¶neriler': [],
            'trend_analizi': '',
            'yÃ¼zde_kontrol': []
        }
        
        # Basit parsing (regex ile bÃ¶lÃ¼mleri ayÄ±r)
        current_section = None
        lines = response_text.split('\n')
        
        total_percent_accumulator: List[int] = []
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # BÃ¶lÃ¼m baÅŸlÄ±klarÄ±nÄ± tespit et
            if 'ğŸ“Š' in line or 'GÃœNLÃœK Ã–ZET' in line:
                current_section = 'gÃ¼nlÃ¼k_Ã¶zet'
                continue
            elif 'âš ï¸' in line or 'SORUNLAR' in line:
                current_section = 'sorunlar'
                continue
            elif 'âœ…' in line or 'Ã‡Ã–ZÃœMLER' in line:
                current_section = 'Ã§Ã¶zÃ¼mler'
                continue
            elif 'ğŸ¯' in line or 'Ã–NERÄ°LER' in line:
                current_section = 'Ã¶neriler'
                continue
            elif 'ğŸ“ˆ' in line or 'TREND' in line:
                current_section = 'trend_analizi'
                continue
            
            # Ä°Ã§eriÄŸi ilgili bÃ¶lÃ¼me ekle
            if current_section:
                if current_section in ['sorunlar', 'Ã§Ã¶zÃ¼mler', 'Ã¶neriler']:
                    if line.startswith('-') or line.startswith('â€¢'):
                        sections[current_section].append(line[1:].strip())
                else:
                    sections[current_section] += line + '\n'
        
            # Basit yÃ¼zde tutarlÄ±lÄ±k yakalama (Ã¶rn: "%15")
            try:
                import re as _re
                matches = _re.findall(r"%(\d{1,3})", line.replace('ï¼…','%'))
                if matches:
                    total_percent_accumulator.extend([int(m) for m in matches])
            except Exception:
                pass
        
        # Normalize/tutarlÄ±lÄ±k notu
        if total_percent_accumulator:
            total = sum([p for p in total_percent_accumulator if 0 <= p <= 100])
            sections['yÃ¼zde_kontrol'].append(f"YÃ¼zde toplamÄ± (ham): %{total}")
        return sections

    def generate_manager_report(self, analysis: Dict, period: str = "gÃ¼nlÃ¼k") -> str:
        """YÃ¶netici iÃ§in Ã¶zet rapor oluÅŸtur"""
        
        if analysis.get('error'):
            return f"âŒ Rapor oluÅŸturulamadÄ±: {analysis['error']}"
        
        structured = analysis['analysis']
        token_info = analysis.get('token_usage', {})
        
        report = f"""
ğŸ­ Ã‡Ä°MENTO FABRÄ°KASI VARDÄ°YA RAPORU
{'='*50}
ğŸ“… Rapor Tipi: {period.title()}
ğŸ• OluÅŸturma: {datetime.now().strftime('%d.%m.%Y %H:%M')}

{structured.get('gÃ¼nlÃ¼k_Ã¶zet', 'Ã–zet bulunamadÄ±')}

âš ï¸ KRÄ°TÄ°K SORUNLAR ({len(structured.get('sorunlar', []))} adet):
"""
        
        for i, sorun in enumerate(structured.get('sorunlar', [])[:5], 1):  # Sadece ilk 5 sorun
            report += f"{i}. {sorun}\n"
        
        report += f"""
ğŸ¯ Ã–NCELIKLI AKSIYONLAR ({len(structured.get('Ã¶neriler', []))} adet):
"""
        
        for i, Ã¶neri in enumerate(structured.get('Ã¶neriler', [])[:3], 1):  # Sadece ilk 3 Ã¶neri
            report += f"{i}. {Ã¶neri}\n"
        
        if token_info:
            report += f"""
ğŸ“Š SÄ°STEM BÄ°LGÄ°SÄ°:
- Token kullanÄ±mÄ±: {token_info.get('total_tokens', 0)}
- Tahmini maliyet: ${token_info.get('estimated_cost', 0):.4f}
"""
        
        return report

    def save_analysis(self, analysis: Dict, filename: str = None) -> str:
        """Analizi dosyaya kaydet"""
        
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"vardiya_analizi_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(analysis, f, ensure_ascii=False, indent=2)
            return filename
        except Exception as e:
            return f"KayÄ±t hatasÄ±: {str(e)}"


# Test fonksiyonu
def test_ai_system():
    """AI sistemini test et"""
    print("ğŸ¤– AI Vardiya Analiz Sistemi Test Ediliyor...")
    
    ai = CimentoVardiyaAI()
    
    # Ã–rnek veri oluÅŸtur (gerÃ§ek veri yerine)
    sample_data = pd.DataFrame({
        'Tarih': ['2025-08-07', '2025-08-07', '2025-08-07'],
        'Vardiya': ['07:00-15:00', '15:00-23:00', '23:00-07:00'],
        'AÃ§Ä±klama': [
            'Ã‡D2 saat 10:30da durdu. AÅŸÄ±rÄ± Ä±sÄ±nma nedeniyle. SoÄŸutma sistemi kontrol edildi.',
            'FÄ±rÄ±n 1 normal Ã§alÄ±ÅŸÄ±yor. CSO deÄŸerleri spek iÃ§inde.',
            'Ã‡Ä°M2 Ã¶ÄŸÃ¼tÃ¼cÃ¼sÃ¼nde titreÅŸim var. BakÄ±m ekibi bilgilendirildi.'
        ],
        'CSO 1': [2.1, 2.3, 2.2],
        'CSO 2': [3.1, 3.0, 3.2]
    })
    
    # Analiz et
    result = ai.analyze_shift_data(sample_data)
    
    if result.get('error'):
        print(f"âŒ Hata: {result['error']}")
        return
    
    # YÃ¶netici raporu oluÅŸtur
    manager_report = ai.generate_manager_report(result)
    print(manager_report)
    
    # Dosyaya kaydet
    saved_file = ai.save_analysis(result)
    print(f"\nğŸ’¾ Analiz kaydedildi: {saved_file}")


if __name__ == "__main__":
    test_ai_system()
