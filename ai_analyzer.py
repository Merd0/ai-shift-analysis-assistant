#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Destekli Vardiya Devir Analiz AsistanÄ±
Ã‡imento FabrikasÄ± Vardiya Defteri Analizi iÃ§in Optimize EdilmiÅŸ AI Sistemi
"""

from openai import OpenAI
import requests
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import json
import re
from config import MODEL_NAME, MAX_TOKENS, TEMPERATURE

class CimentoVardiyaAI:
    def __init__(self, api_key: str = "", provider: str = "openai", model: Optional[str] = None, base_url: Optional[str] = None):
        """
        Ã‡imento fabrikasÄ± vardiya analizi iÃ§in AI sistemi
        
        Args:
            api_key: OpenAI API key (gÃ¼venlik iÃ§in parametre olarak alÄ±nÄ±r)
        """
        if not api_key:
            raise ValueError("âš ï¸ API Key gerekli! LÃ¼tfen GUI'de API key'inizi girin.")

        # SaÄŸlayÄ±cÄ± seÃ§imi
        self.provider = provider.lower().strip()
        self.api_key = api_key

        # OpenAI istemcisi (varsayÄ±lan)
        self.client = None
        if self.provider == "openai":
            self.client = OpenAI(api_key=api_key)

        # Model seÃ§imi
        self.model = model or MODEL_NAME
        self.max_tokens = MAX_TOKENS
        self.temperature = TEMPERATURE
        
        # Ã‡imento fabrikasÄ± spesifik context
        self.cement_context = self._load_cement_context()
        
    def _load_cement_context(self) -> str:
        """Ã‡imento fabrikasÄ± iÃ§in optimize edilmiÅŸ context"""
        return """
# Ã‡Ä°MENTO FABRÄ°KASI VARDÄ°YA DEFTERÄ° ANALÄ°Z SÄ°STEMÄ°

## AMAÃ‡:
Ã‡imento Ã¼retim sÃ¼recindeki vardiya kayÄ±tlarÄ±nÄ± analiz ederek:
- Ãœretim sorunlarÄ±nÄ± tespit etmek
- Ekipman arÄ±zalarÄ±nÄ± kategorize etmek  
- Ã‡Ã¶zÃ¼m Ã¶nerileri sunmak
- YÃ¶netici raporlarÄ± oluÅŸturmak

## Ã‡Ä°MENTO ÃœRETÄ°M SÃœRECÄ° BÄ°LGÄ°SÄ°:
1. HAMMADDE HAZIRLAMA: KireÃ§taÅŸÄ±, kil, demir cevheri karÄ±ÅŸÄ±mÄ±
2. Ã–ÄÃœTME: Ham karÄ±ÅŸÄ±m Ã¶ÄŸÃ¼tÃ¼cÃ¼lerde (Ã‡D1, Ã‡D2, Ã‡D3, Ã‡D4) ince hale getirilir
3. FIRINDA YAKMA: 1450Â°C'de klinker Ã¼retimi (FÄ±rÄ±n 1, FÄ±rÄ±n 2)
4. Ã‡Ä°MENTO Ã–ÄÃœTME: Klinker + alÃ§Ä± karÄ±ÅŸÄ±mÄ± (Ã‡Ä°M1, Ã‡Ä°M2)
5. DEPOLAMA: Silolarda (Silo 1-8) depolama
6. PAKETLEME/YÃœKLEME: Torba veya dÃ¶kme sevkiyat

## TEMEL EKÄ°PMANLAR:
- Ã‡D1,Ã‡D2,Ã‡D3,Ã‡D4: Ã‡iÄŸ deÄŸirmenler
- Ã‡Ä°M1,Ã‡Ä°M2: Ã‡imento deÄŸirmenleri  
- F1,F2: FÄ±rÄ±nlar
- Silo 1-8: Depolama silolarÄ±
- CSO: Ã‡imento kalite parametreleri
- XRF: Kimyasal analiz cihazÄ±
- Bunker, KonveyÃ¶r, Filtre sistemleri

## SIKÃ‡A YAÅANAN SORUNLAR:
- AÅŸÄ±rÄ± Ä±sÄ±nma (Ã¶zellikle deÄŸirmenlerde)
- Filtre tÄ±kanmasÄ±
- KonveyÃ¶r arÄ±zalarÄ±
- Kalite spek dÄ±ÅŸÄ± deÄŸerler
- Hammadde besleme sorunlarÄ±
- Elektrik kesintileri
- Yedek parÃ§a eksiklikleri

## ANALÄ°Z Ã‡IKTI FORMATI:
1. GÃœNLÃœK Ã–ZET: Ãœretim miktarÄ±, duruÅŸ sÃ¼releri
2. SORUNLAR: YaÅŸanan arÄ±zalar, nedenleri, sÃ¼releri
3. Ã‡Ã–ZÃœMLER: AlÄ±nan aksiyonlar, etkililik
4. Ã–NERÄ°LER: Ã–nleyici bakÄ±m, iyileÅŸtirme Ã¶nerileri
5. TREND: Tekrarlanan sorunlar, risk analizi
"""

    def analyze_shift_data(self, data: pd.DataFrame, date_range: str = "gÃ¼nlÃ¼k", 
                          analysis_options: List[str] = None, user_question: str = "") -> Dict:
        """
        Vardiya verilerini geliÅŸmiÅŸ AI sistemi ile analiz et
        
        Args:
            data: Analiz edilecek vardiya verileri
            date_range: Analiz periyodu ("gÃ¼nlÃ¼k", "haftalÄ±k", vb.)
            analysis_options: Ä°stenilen rapor bÃ¶lÃ¼mleri listesi
            user_question: KullanÄ±cÄ±nÄ±n Ã¶zel sorusu
            
        Returns:
            Dict: AI analiz sonuÃ§larÄ±
        """
        
        # Veriyi Ã¶zetleyerek token tasarrufu
        summary_data = self._summarize_data(data)
        
        # Yeni geliÅŸmiÅŸ AI prompt oluÅŸtur
        prompt = self._create_analysis_prompt(summary_data, date_range, analysis_options, user_question)
        
        # AI analizi Ã§aÄŸÄ±r
        analysis = self._call_llm_api(prompt)
        
        return analysis

    def _summarize_data(self, data: pd.DataFrame) -> str:
        """Veriyi zengin ÅŸekilde Ã¶zetleyip AI'a gÃ¼Ã§lÃ¼ baÄŸlam saÄŸla (KPI + trend + top listeler).

        AyrÄ±ca bazÄ± daÄŸÄ±lÄ±mlarÄ± Ã¶nceden hesaplayÄ±p yÃ¼zde toplamÄ±nÄ± %100'e normalize eder.
        """

        lines: List[str] = []

        # Tarih bilgisi (varsa)
        date_col_candidates = [c for c in data.columns if any(k in c.lower() for k in ['tarih', 'date'])]
        date_range_text = "N/A"
        if date_col_candidates:
            dc = date_col_candidates[0]
            try:
                dates = pd.to_datetime(data[dc], errors='coerce')
                min_d = dates.min()
                max_d = dates.max()
                if pd.notna(min_d) and pd.notna(max_d):
                    date_range_text = f"{min_d.date()} - {max_d.date()}"
            except Exception:
                pass

        # Genel
        lines.append("ğŸ“Š GENEL BÄ°LGÄ°:")
        lines.append(f"- Toplam kayÄ±t: {len(data)}")
        lines.append(f"- Tarih aralÄ±ÄŸÄ±: {date_range_text}")

        # Vardiya daÄŸÄ±lÄ±mÄ± (normalize + Ã¶rnekleme hatalarÄ±na dayanÄ±klÄ±)
        shift_col = next((c for c in data.columns if 'vardiya' in c.lower()), None)
        if shift_col is not None:
            try:
                vc = data[shift_col].astype(str).str.strip().replace({'': None}).dropna().value_counts()
                dist = vc.head(10)
                lines.append("\nğŸ•’ VARDÄ°YA DAÄILIMI (ilk 10):")
                for k, v in dist.items():
                    lines.append(f"- {k}: {int(v)}")
            except Exception:
                pass

        # Normalize edici yardÄ±mcÄ±
        def _normalized_percentages(vc: pd.Series, top_n: int = 10) -> List[Tuple[str, int, int]]:
            """value_counts serisini ilk top_n iÃ§in (ad, adet, %) ve bir 'DiÄŸer' ile %100'e
            tamamlayarak dÃ¶ndÃ¼rÃ¼r. YÃ¼zdeler tamsayÄ±ya yuvarlanÄ±r ve son kaleme fark eklenir.
            """
            items = vc.head(top_n)
            total = int(vc.sum()) if vc.sum() else 0
            result: List[Tuple[str, int, int]] = []
            if total == 0:
                return result
            percents = []
            names = list(items.index.astype(str))
            counts = list(items.astype(int).values)
            for c in counts:
                percents.append(int(round(c * 100.0 / total)))
            diff = 100 - sum(percents)
            if percents:
                percents[-1] += diff
            for n, c, p in zip(names, counts, percents):
                result.append((n, int(c), int(p)))
            # DiÄŸer
            others = total - sum(counts)
            if others > 0:
                p_other = max(0, 100 - sum([p for _, _, p in result]))
                result.append(("DiÄŸer", int(others), int(p_other)))
            return result

        # Ekipman daÄŸÄ±lÄ±mÄ±
        equipment_col = next((c for c in data.columns if any(k in c.lower() for k in ['ekipman', 'makine', 'Ã¼nite', 'unite', 'unit'])), None)
        if equipment_col is not None:
            try:
                vc = data[equipment_col].astype(str).str.strip().replace({'': None}).dropna().value_counts()
                lines.append("\nğŸ­ EKÄ°PMAN DAÄILIMI (ilk 10, normalize):")
                dist = _normalized_percentages(vc, top_n=10)
                for name, cnt, pct in dist:
                    lines.append(f"- {name}: {cnt} kayÄ±t (%{pct})")
                if vc.sum() > 0:
                    lines.append("Toplam = %100")
            except Exception:
                pass

        # Sorun / Kategori daÄŸÄ±lÄ±mÄ±
        issue_col = next((c for c in data.columns if any(k in c.lower() for k in ['sorun', 'arÄ±za', 'ariza', 'problem', 'kategori'])), None)
        if issue_col is not None:
            try:
                vc = data[issue_col].astype(str).str.strip().replace({'': None}).dropna().str.lower().value_counts()
                lines.append("\nâš ï¸ SORUN KATEGORÄ°LERÄ° (ilk 10, normalize):")
                dist = _normalized_percentages(vc, top_n=10)
                for name, cnt, pct in dist:
                    lines.append(f"- {name}: {cnt} kayÄ±t (%{pct})")
                if vc.sum() > 0:
                    lines.append("Toplam = %100")
            except Exception:
                pass

        # DuruÅŸ/SÃ¼re (dakika)
        duration_col = next((c for c in data.columns if any(k in c.lower() for k in ['sÃ¼re', 'sure', 'dakika', 'dk'])), None)
        if duration_col is not None:
            try:
                # Metin iÃ§indeki sayÄ±larÄ± da yakalamaya Ã§alÄ±ÅŸ (Ã¶r. "45 dk", "~30")
                cleaned = (
                    data[duration_col]
                    .astype(str)
                    .str.extract(r'(\d+[\.,]?\d*)', expand=False)
                )
                durations = pd.to_numeric(cleaned.str.replace(',', '.', regex=False), errors='coerce')
                total_min = durations.fillna(0).sum()
                avg_min = durations.dropna().mean() if durations.notna().any() else 0
                lines.append("\nâ±ï¸ DuruÅŸ SÃ¼resi (dakika):")
                lines.append(f"- Toplam: {int(total_min)} dk")
                lines.append(f"- Ortalama: {avg_min:.1f} dk/kayÄ±t")
            except Exception:
                pass

        # Trend Ã¶zeti (son 7 gÃ¼n vs Ã¶nceki 7 gÃ¼n)
        if date_col_candidates:
            dc = date_col_candidates[0]
            try:
                df_copy = data.copy()
                df_copy[dc] = pd.to_datetime(df_copy[dc], errors='coerce')
                daily_counts = df_copy.groupby(df_copy[dc].dt.date).size().sort_index()
                if len(daily_counts) >= 14:
                    last7 = daily_counts[-7:].sum()
                    prev7 = daily_counts[-14:-7].sum()
                    delta = last7 - prev7
                    trend = "â†‘" if delta > 0 else ("â†“" if delta < 0 else "=")
                    lines.append("\nğŸ“ˆ Trend (kayÄ±t adedi):")
                    lines.append(f"- Son 7 gÃ¼n: {int(last7)} | Ã–nceki 7: {int(prev7)} | Fark: {int(delta)} {trend}")
            except Exception:
                pass

        # AÃ§Ä±klamalardan kÄ±sa Ã¶rnekler
        description_columns = [col for col in data.columns if any(keyword in col.lower() for keyword in ['aÃ§Ä±klama', 'aciklama', 'iletil', 'takip', 'kalite', 'arÄ±za', 'ariza', 'bakÄ±m', 'bakim', 'not', 'yorum'])]
        if description_columns:
            lines.append("\nğŸ” Ã–RNEK KAYITLAR (max 3 kolon x 3 Ã¶rnek):")
            for col in description_columns[:3]:
                non_empty = data[col].dropna().astype(str)
                if len(non_empty) > 0:
                    samples = non_empty.head(3).tolist()
                    lines.append(f"- {col}:")
                    for i, sample in enumerate(samples, 1):
                        sample_text = sample[:200] + "..." if len(sample) > 200 else sample
                        lines.append(f"  {i}. {sample_text}")

        return "\n".join(lines)

    def _create_analysis_prompt(self, summary_data: str, date_range: str, analysis_options: List[str] = None, user_question: str = "") -> str:
        """Yeni geliÅŸmiÅŸ prompt sistemi ile analiz prompt'u oluÅŸtur"""
        
        # Yeni prompt sistemini import et
        from prompts import SYSTEM_PROMPT, USER_PROMPT_TEMPLATE
        
        # VarsayÄ±lan analiz seÃ§enekleri
        if analysis_options is None:
            analysis_options = [
                "ğŸ¯ YÃ¶netici Ã–zeti",
                "ğŸ“Š Performans Karnesi", 
                "ğŸ” KÃ¶k Neden Analizi",
                "ğŸ“ˆ Zaman Trendleri ve Risk Tahmini",
                "ğŸ’¡ SMART Eylem PlanÄ±",
                "ğŸ“Œ YÃ¶netici Aksiyon Panosu"
            ]
        
        # Analiz seÃ§eneklerini formatla
        formatted_options = "\n".join([f"âœ… {option}" for option in analysis_options])
        
        # Yeni prompt template'ini kullan
        user_prompt = USER_PROMPT_TEMPLATE.format(
            data_summary=summary_data,
            analysis_options=formatted_options,
            user_question=user_question if user_question else "Yok"
        )
        
        # System prompt + User prompt kombinasyonu
        full_prompt = f"{SYSTEM_PROMPT}\n\n{user_prompt}"
        
        return full_prompt

    def _call_llm_api(self, prompt: str) -> Dict:
        """SeÃ§ili saÄŸlayÄ±cÄ±ya gÃ¶re API Ã§aÄŸrÄ±sÄ±"""

        try:
            if self.provider == "openai":
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=self.max_tokens,
                    temperature=0.8,
                    top_p=0.95,
                    frequency_penalty=0.6,
                    presence_penalty=0.6,
                )
                analysis_text = response.choices[0].message.content
                token_usage = {
                    'prompt_tokens': getattr(response.usage, 'prompt_tokens', None),
                    'completion_tokens': getattr(response.usage, 'completion_tokens', None),
                    'total_tokens': getattr(response.usage, 'total_tokens', None),
                }

            elif self.provider == "anthropic":
                # Claude Messages API
                url = "https://api.anthropic.com/v1/messages"
                headers = {
                    "x-api-key": self.api_key,
                    "anthropic-version": "2023-06-01",
                    "content-type": "application/json"
                }
                payload = {
                    "model": self.model,
                    "max_tokens": self.max_tokens,
                    "temperature": 0.8,
                    "messages": [{"role": "user", "content": prompt}]
                }
                r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=60)
                r.raise_for_status()
                data = r.json()
                # Claude yanÄ±tÄ±
                parts = data.get("content", [])
                analysis_text = "".join([p.get("text", "") for p in parts]) if isinstance(parts, list) else data.get("content", "")
                token_usage = data.get("usage", {})

            elif self.provider == "xai":
                # xAI Grok (OpenAI uyumlu style olabilir; burada basit REST Ã¶rneÄŸi)
                base = "https://api.x.ai/v1"
                url = f"{base}/chat/completions"
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                }
                payload = {
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": self.max_tokens,
                    "temperature": 0.8
                }
                r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=60)
                r.raise_for_status()
                data = r.json()
                analysis_text = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                token_usage = data.get("usage", {})

            else:
                raise ValueError(f"Desteklenmeyen saÄŸlayÄ±cÄ±: {self.provider}")

            structured_analysis = self._parse_analysis_response(analysis_text)
            return {
                'analysis': structured_analysis,
                'raw_response': analysis_text,
                'token_usage': token_usage,
                'timestamp': datetime.now().isoformat()
            }
        except Exception as e:
            return {
                'error': f"AI analizi hatasÄ±: {str(e)}",
                'analysis': None,
                'token_usage': None,
                'timestamp': datetime.now().isoformat()
            }

    def _parse_analysis_response(self, response_text: str) -> Dict:
        """AI yanÄ±tÄ±nÄ± yapÄ±landÄ±rÄ±lmÄ±ÅŸ formata Ã§evir"""
        
        sections = {
            'gÃ¼nlÃ¼k_Ã¶zet': '',
            'sorunlar': [],
            'Ã§Ã¶zÃ¼mler': [],
            'Ã¶neriler': [],
            'trend_analizi': '',
            'yÃ¼zde_kontrol': []
        }
        
        # Basit parsing (regex ile bÃ¶lÃ¼mleri ayÄ±r)
        current_section = None
        lines = response_text.split('\n')
        
        total_percent_accumulator: List[int] = []
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # BÃ¶lÃ¼m baÅŸlÄ±klarÄ±nÄ± tespit et
            if 'ğŸ“Š' in line or 'GÃœNLÃœK Ã–ZET' in line:
                current_section = 'gÃ¼nlÃ¼k_Ã¶zet'
                continue
            elif 'âš ï¸' in line or 'SORUNLAR' in line:
                current_section = 'sorunlar'
                continue
            elif 'âœ…' in line or 'Ã‡Ã–ZÃœMLER' in line:
                current_section = 'Ã§Ã¶zÃ¼mler'
                continue
            elif 'ğŸ¯' in line or 'Ã–NERÄ°LER' in line:
                current_section = 'Ã¶neriler'
                continue
            elif 'ğŸ“ˆ' in line or 'TREND' in line:
                current_section = 'trend_analizi'
                continue
            
            # Ä°Ã§eriÄŸi ilgili bÃ¶lÃ¼me ekle
            if current_section:
                if current_section in ['sorunlar', 'Ã§Ã¶zÃ¼mler', 'Ã¶neriler']:
                    if line.startswith('-') or line.startswith('â€¢'):
                        sections[current_section].append(line[1:].strip())
                else:
                    sections[current_section] += line + '\n'
        
            # Basit yÃ¼zde tutarlÄ±lÄ±k yakalama (Ã¶rn: "%15")
            try:
                import re as _re
                matches = _re.findall(r"%(\d{1,3})", line.replace('ï¼…','%'))
                if matches:
                    total_percent_accumulator.extend([int(m) for m in matches])
            except Exception:
                pass
        
        # Normalize/tutarlÄ±lÄ±k notu
        if total_percent_accumulator:
            total = sum([p for p in total_percent_accumulator if 0 <= p <= 100])
            sections['yÃ¼zde_kontrol'].append(f"YÃ¼zde toplamÄ± (ham): %{total}")
        return sections

    def generate_manager_report(self, analysis: Dict, period: str = "gÃ¼nlÃ¼k") -> str:
        """YÃ¶netici iÃ§in Ã¶zet rapor oluÅŸtur"""
        
        if analysis.get('error'):
            return f"âŒ Rapor oluÅŸturulamadÄ±: {analysis['error']}"
        
        structured = analysis['analysis']
        token_info = analysis.get('token_usage', {})
        
        report = f"""
ğŸ­ Ã‡Ä°MENTO FABRÄ°KASI VARDÄ°YA RAPORU
{'='*50}
ğŸ“… Rapor Tipi: {period.title()}
ğŸ• OluÅŸturma: {datetime.now().strftime('%d.%m.%Y %H:%M')}

{structured.get('gÃ¼nlÃ¼k_Ã¶zet', 'Ã–zet bulunamadÄ±')}

âš ï¸ KRÄ°TÄ°K SORUNLAR ({len(structured.get('sorunlar', []))} adet):
"""
        
        for i, sorun in enumerate(structured.get('sorunlar', [])[:5], 1):  # Sadece ilk 5 sorun
            report += f"{i}. {sorun}\n"
        
        report += f"""
ğŸ¯ Ã–NCELIKLI AKSIYONLAR ({len(structured.get('Ã¶neriler', []))} adet):
"""
        
        for i, Ã¶neri in enumerate(structured.get('Ã¶neriler', [])[:3], 1):  # Sadece ilk 3 Ã¶neri
            report += f"{i}. {Ã¶neri}\n"
        
        if token_info:
            report += f"""
ğŸ“Š SÄ°STEM BÄ°LGÄ°SÄ°:
- Token kullanÄ±mÄ±: {token_info.get('total_tokens', 0)}
- Tahmini maliyet: ${token_info.get('estimated_cost', 0):.4f}
"""
        
        return report

    def save_analysis(self, analysis: Dict, filename: str = None) -> str:
        """Analizi dosyaya kaydet"""
        
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"vardiya_analizi_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(analysis, f, ensure_ascii=False, indent=2)
            return filename
        except Exception as e:
            return f"KayÄ±t hatasÄ±: {str(e)}"


# Test fonksiyonu
def test_ai_system():
    """AI sistemini test et"""
    print("ğŸ¤– AI Vardiya Analiz Sistemi Test Ediliyor...")
    
    ai = CimentoVardiyaAI()
    
    # Ã–rnek veri oluÅŸtur (gerÃ§ek veri yerine)
    sample_data = pd.DataFrame({
        'Tarih': ['2025-08-07', '2025-08-07', '2025-08-07'],
        'Vardiya': ['07:00-15:00', '15:00-23:00', '23:00-07:00'],
        'AÃ§Ä±klama': [
            'Ã‡D2 saat 10:30da durdu. AÅŸÄ±rÄ± Ä±sÄ±nma nedeniyle. SoÄŸutma sistemi kontrol edildi.',
            'FÄ±rÄ±n 1 normal Ã§alÄ±ÅŸÄ±yor. CSO deÄŸerleri spek iÃ§inde.',
            'Ã‡Ä°M2 Ã¶ÄŸÃ¼tÃ¼cÃ¼sÃ¼nde titreÅŸim var. BakÄ±m ekibi bilgilendirildi.'
        ],
        'CSO 1': [2.1, 2.3, 2.2],
        'CSO 2': [3.1, 3.0, 3.2]
    })
    
    # Analiz et
    result = ai.analyze_shift_data(sample_data)
    
    if result.get('error'):
        print(f"âŒ Hata: {result['error']}")
        return
    
    # YÃ¶netici raporu oluÅŸtur
    manager_report = ai.generate_manager_report(result)
    print(manager_report)
    
    # Dosyaya kaydet
    saved_file = ai.save_analysis(result)
    print(f"\nğŸ’¾ Analiz kaydedildi: {saved_file}")


if __name__ == "__main__":
    test_ai_system()
